{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT_NAME = os.getenv(\"LANGCHAIN_PROJECT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
    "LANGCHAIN_TRACING_V2=\"true\"\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=LANGCHAIN_API_KEY\n",
    "LANGCHAIN_PROJECT=LANGCHAIN_PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-1.0-pro'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"gemini-1.0-pro\"\n",
    "\"Gemma2-9b-It\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InMemoryChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"my_new_memory_chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_memory = RunnableWithMessageHistory(runnable=llm, get_session_history=get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Dinesh!  \\n\\nIs there anything I can help you with? üòä \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_time': 0.038181818, 'prompt_time': 8.2899e-05, 'queue_time': 0.013406251000000001, 'total_time': 0.038264717}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7698f63a-2d19-4aef-b5a2-adb145af4124-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_with_memory.invoke(input=(\"my name is dinesh\"), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Dinesh.  \\n\\nI remember that! üòÑ \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 47, 'total_tokens': 75, 'completion_time': 0.050909091, 'prompt_time': 0.001444956, 'queue_time': 0.011613703, 'total_time': 0.052354047}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2bae38ef-ef99-4363-a3c4-746886def50d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 28, 'total_tokens': 75})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_memory.invoke(input=(\"what is my name\"), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='my name is dinesh', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello, Dinesh!  \\n\\nIs there anything I can help you with? üòä \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_time': 0.038181818, 'prompt_time': 8.2899e-05, 'queue_time': 0.013406251000000001, 'total_time': 0.038264717}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7698f63a-2d19-4aef-b5a2-adb145af4124-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35}),\n",
       " HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Dinesh.  \\n\\nI remember that! üòÑ \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 47, 'total_tokens': 75, 'completion_time': 0.050909091, 'prompt_time': 0.001444956, 'queue_time': 0.011613703, 'total_time': 0.052354047}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2bae38ef-ef99-4363-a3c4-746886def50d-0', usage_metadata={'input_tokens': 47, 'output_tokens': 28, 'total_tokens': 75})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store['my_new_memory_chat'].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=\"The Illustrated Transformer.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len)\n",
    "split_docs = text_splitter.split_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(documents=split_docs, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 5, 'source': 'The Illustrated Transformer.pdf'}, page_content='Self-Attention in Detail\\nLet‚Äôs first look at how to calculate self-attention using vectors, then proceed to look at how it‚Äôs actually implemented ‚Äìusing matrices.'),\n",
       " Document(metadata={'page': 5, 'source': 'The Illustrated Transformer.pdf'}, page_content='Self-Attention in Detail\\nLet‚Äôs first look at how to calculate self-attention using vectors, then proceed to look at how it‚Äôs actually implemented ‚Äìusing matrices.'),\n",
       " Document(metadata={'page': 4, 'source': 'The Illustrated Transformer.pdf'}, page_content='What does ‚Äúit‚Äù in this sentence refer to? Is it referring to the street or to the animal? It‚Äôs a simple question to a human,but not as simple to an algorithm.\\nWhen the model is processing the word ‚Äúit‚Äù, self-attention allows it to associate ‚Äúit‚Äù with ‚Äúanimal‚Äù.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at otherpositions in the input sequence for clues that can help lead to a better encoding for this word.'),\n",
       " Document(metadata={'page': 4, 'source': 'The Illustrated Transformer.pdf'}, page_content='What does ‚Äúit‚Äù in this sentence refer to? Is it referring to the street or to the animal? It‚Äôs a simple question to a human,but not as simple to an algorithm.\\nWhen the model is processing the word ‚Äúit‚Äù, self-attention allows it to associate ‚Äúit‚Äù with ‚Äúanimal‚Äù.\\nAs the model processes each word (each position in the input sequence), self attention allows it to look at otherpositions in the input sequence for clues that can help lead to a better encoding for this word.')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"what is self attention mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 5, 'source': 'The Illustrated Transformer.pdf'}, page_content='Self-Attention in Detail\\nLet‚Äôs first look at how to calculate self-attention using vectors, then proceed to look at how it‚Äôs actually implemented ‚Äìusing matrices.'),\n",
       " Document(metadata={'page': 5, 'source': 'The Illustrated Transformer.pdf'}, page_content='Self-Attention in Detail\\nLet‚Äôs first look at how to calculate self-attention using vectors, then proceed to look at how it‚Äôs actually implemented ‚Äìusing matrices.'),\n",
       " Document(metadata={'page': 8, 'source': 'The Illustrated Transformer.pdf'}, page_content='That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neuralnetwork. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let‚Äôslook at that now that we‚Äôve seen the intuition of the calculation on the word level.\\nMatrix Calculation of Self-Attention')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is self-attention mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the following context:\n",
    "{context}\n",
    "\n",
    "Answer: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_chain = (RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    " | prompt\n",
    " | llm\n",
    " | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Self-attention is a mechanism in machine learning, particularly in natural language processing, that allows a model to weigh the importance of different words in a sequence when processing that sequence.  \\n\\nHere's a breakdown based on the provided text:\\n\\n* **Purpose:** Self-attention helps a model understand the relationships between words in a sentence or paragraph. Some words might be more important for understanding the overall meaning than others.\\n* **Vector-based Calculation (Conceptual):**  The text mentions calculating self-attention using vectors. This involves representing each word as a vector and then using mathematical operations to compare the similarity between word vectors. Words that are similar in meaning or context will have vectors that are closer together.\\n* **Matrix-based Calculation (Practical):** The actual implementation of self-attention uses matrices for efficiency. Matrices allow for faster and more scalable calculations compared to working with individual vectors, especially when dealing with long sequences of words.\\n\\n**Key Points:**\\n\\n* **Weighted Importance:** Self-attention assigns different weights to words based on their relevance.\\n* **Contextual Understanding:** It enables models to capture the context of words within a sequence.\\n* **Transformer Architectures:** Self-attention is a core component of transformer models, which have revolutionized natural language processing tasks.\\n\\n\\nLet me know if you'd like a more detailed explanation of the vector or matrix calculations involved! \\n\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain.invoke(\"what is self-attention mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
